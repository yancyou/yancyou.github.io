Welcome to my homepage. I am a junior undergraduate student majoring in **[<span style="color:#0400ff">Computer Science</span>](https://www.cs.wisc.edu/)** (**[<span style="color:#0400ff">UW-Madison, CDIS</span>](https://cdis.wisc.edu/)**, CS#13) advised by Prof. [<span style="color:#0400ff">Hanbaek Lyu</span>](https://hanbaeklyu.com/) and Prof. [<span style="color:#0400ff">Junjie Hu</span>](https://junjiehu.github.io/). My research/internship experience covers [<span style="color:#0400ff">Shanghai Jiao Tong University Epic Lab</span>](https://github.com/SJTU-EPIC-Lab) (advised by Prof. [<span style="color:#0400ff">Linfeng Zhang</span>](http://www.zhanglinfeng.tech/) and work with my mentor [<span style="color:#0400ff">Shaobo Wang</span>](https://gszfwsb.github.io/)). University College London (work with Dr. [<span style="color:#0400ff">Yujian Gan</span>](https://profiles.ucl.ac.uk/100730-yujian-gan)). I'm also a machine learning research assistant in [<span style="color:#0400ff">UW-Madison Marler Lab</span>](https://marlerlab.psych.wisc.edu/)(advised by Prof. [<span style="color:#0400ff">Catherine Marler</span>](https://marlerlab.psych.wisc.edu/staff/marler-catherine/)). I am very grateful to the above institutions, mentors and collaborators.

#### Research Interests

My current passion revolves around building AI systems that not only perceive and generate across modalities, but also reason, act, and learn from the world in a data-driven, tool-empowered fashion—advancing us toward truly intelligent, context-aware agents. Which includes:

1. **Multimodal LLMs**  

    Build scalable transformer architectures that integrate visual, textual, and auditory modalities—leveraging modality-specific encoders and joint training objectives—to enable unified understanding and generation across diverse sensory inputs.
    
    **Related Papers**: [<span style="color:#0400ff">A Survey on Multimodal Large Language Models</span>](https://arxiv.org/pdf/2306.13549)

2. **Agent Tool Use**  

    Design intelligent agent frameworks where LLMs plan and orchestrate external API calls, database queries, and code execution—incorporating dynamic tool selection, stateful memory, and error-handling—to accomplish complex, multi-step tasks autonomously.
    
    **Related Papers**: [<span style="color:#0400ff">Gorilla</span>](https://arxiv.org/pdf/2305.15334) | [<span style="color:#0400ff">ToolRL</span>](https://arxiv.org/pdf/2504.13958)

3. **Multimodal Reasoning**  

    Develop cross-modal reasoning frameworks that align and fuse heterogeneous signals—using hierarchical attention, graph-based representations, and consistency objectives—to enable agents to perform robust inference and decision-making across visual, textual, and auditory data streams.
    
    **Related Papers**: [<span style="color:#0400ff">Reason-RFT</span>](https://arxiv.org/pdf/2503.20752) | [<span style="color:#0400ff">Video-R1</span>](https://arxiv.org/pdf/2503.21776)

4. **Data-Centric AI**  

    Build end-to-end pipelines for curating, validating, and augmenting datasets—measuring coverage, balance, and consistency—to ensure every model component is trained on high-quality, representative examples.
    
    **Related Papers**: [<span style="color:#0400ff">RDED</span>](https://arxiv.org/pdf/2312.03526) | [<span style="color:#0400ff">NCFM</span>](https://arxiv.org/pdf/2502.20653)

#### Contact<p id="contact-info"></p>

* I am very enthusiastic about discussing academic issues or any interesting project-related topics! If you'd like to engage in a discussion or collaborate, feel free to contact me via email at any time. 

* ✉️ [<span style="color:#0400ff">xouyang7@wisc.edu</span>](mailto:xouyang7@wisc.edu)
